library(datasets)  # Load/unload base packages manually
?mtcars
head(mtcars)
# Good to first check univariate distributions
hist(mtcars$wt)
hist(mtcars$mpg)
# Basic X-Y plot for two quantitative variables
plot(mtcars$wt, mtcars$mpg)
# Add some options
plot(mtcars$wt, mtcars$mpg,
pch = 19,         # Solid circle
cex = 1.5,        # Make 150% size
col = "#cc0000",  # Red
main = "MPG as a Function of Weight of Cars",
xlab = "Weight (in 1000 pounds)",
ylab = "MPG")
# Clear packages
detach("package:datasets", unload = TRUE)  # For base
# Clear plots
dev.off()  # But only if there IS a plot
# Clear console
cat("\014")  # ctrl+L
library(datasets)  # Load/unload base packages manually
# Annual Canadian Lynx trappings 1821-1934
?lynx
head(lynx)
# Default
hist(lynx)
# Add some options
hist(lynx,
breaks = 14,          # "Suggests" 14 bins
freq   = FALSE,       # Axis shows density, not freq.
col    = "thistle1",  # Color for histogram
main   = paste("Histogram of Annual Canadian Lynx",
"Trappings, 1821-1934"),
xlab   = "Number of Lynx Trapped")
# Add a normal distribution
curve(dnorm(x, mean = mean(lynx), sd = sd(lynx)),
col = "thistle4",  # Color of curve
lwd = 2,           # Line width of 2 pixels
add = TRUE)        # Superimpose on previous graph
# Add two kernel density estimators
lines(density(lynx), col = "blue", lwd = 2)
lines(density(lynx, adjust = 3), col = "purple", lwd = 2)
# Add a rug plot
rug(lynx, lwd = 2, col = "gray")
# Clear packages
detach("package:datasets", unload = TRUE)  # For base
# Clear plots
dev.off()  # But only if there IS a plot
# Clear console
cat("\014")  # ctrl+L
library(datasets)  # Load/unload base packages manually
head(iris)
summary(iris$Species)       # Categorical variable
summary(iris$Sepal.Length)  # Quantitative variable
summary(iris)               # Entire data frame
detach("package:datasets", unload = TRUE)   # For base
# Clear console
cat("\014")  # ctrl+L
library(datasets)  # Load base packages manually
# Installs pacman ("package manager") if needed
if (!require("pacman")) install.packages("pacman")
# Use pacman to load add-on packages as desired
pacman::p_load(pacman, psych)
head(iris)
# Get info on package
p_help(psych)           # Opens package PDF in browser
# Get info on package
p_help(psych)           # Opens package PDF in browser
# Use pacman to load add-on packages as desired
pacman::p_load(pacman, psych)
head(iris)
# Get info on package
p_help(psych)           # Opens package PDF in browser
p_help(psych, web = F)  # Opens help in R Viewer
describe(iris$Sepal.Length)  # One quantitative variable
describe(iris)               # Entire data frame
# Clear packages
detach("package:datasets", unload = TRUE)  # For base
# Clear plots
dev.off()  # But only if there IS a plot
# Clear console
cat("\014")  # ctrl+L
# Clear environment
rm(list = ls())
# Clear packages
detach("package:datasets", unload = TRUE)  # For base
# Clear plots
dev.off()  # But only if there IS a plot
# Clear console
cat("\014")  # ctrl+L
library(datasets)  # Load/unload base packages manually
head(iris)
hist(iris$Petal.Length)
summary(iris$Petal.Length)
summary(iris$Species)  # Get names and n for each species
# Versicolor
hist(iris$Petal.Length[iris$Species == "versicolor"],
main = "Petal Length: Versicolor")
# Virginica
hist(iris$Petal.Length[iris$Species == "virginica"],
main = "Petal Length: Virginica")
# Setosa
hist(iris$Petal.Length[iris$Species == "setosa"],
main = "Petal Length: Setosa")
# Short petals only (all Setosa)
hist(iris$Petal.Length[iris$Petal.Length < 2],
main = "Petal Length < 2")
# Short Virginica petals only
hist(iris$Petal.Length[iris$Species == "virginica" &
iris$Petal.Length < 5.5],
main = "Petal Length: Short Virginica")
# Format: data[rows, columns]
# Leave rows or columns blank to select all
i.setosa <- iris[iris$Species == "setosa", ]
head(i.setosa)
summary(i.setosa$Petal.Length)
hist(i.setosa$Petal.Length)
# Clear environment
rm(list = ls())
# Clear packages
detach("package:datasets", unload = TRUE)  # For base
# Clear plots
dev.off()  # But only if there IS a plot
# Clear console
cat("\014")  # ctrl+L
n1 <- 15  # Double precision by default
n1
typeof(n1)
n2 <- 1.5
n2
typeof(n2)
c1 <- "c"
c1
typeof(c1)
c2 <- "a string of text"
c2
typeof(c2)
l1 <- TRUE
l1
typeof(l1)
l2 <- F
l2
typeof(l2)
v1 <- c(1, 2, 3, 4, 5)
v1
is.vector(v1)
v2 <- c("a", "b", "c")
v2
is.vector(v2)
v3 <- c(TRUE, TRUE, FALSE, FALSE, TRUE)
v3
is.vector(v3)
m1 <- matrix(c(T, T, F, F, T, F), nrow = 2)
m1
m2 <- matrix(c("a", "b",
"c", "d"),
nrow = 2,
byrow = T)
m2
m2 <- matrix(c("a", "b",
"c", "d"),
nrow = 2,
byrow = F)
m2
m2 <- matrix(c("a", "b",
"c", "d"),
nrow = 2,
byrow = T)
m2
# Give data, then dimemensions (rows, columns, tables)
a1 <- array(c( 1:24), c(4, 3, 2))
a1
# Give data, then dimemensions (rows, columns, tables)
a1 <- array(c( 1:24), c(4, 3, 2))
a1
vNumeric   <- c(1, 2, 3)
vCharacter <- c("a", "b", "c")
vLogical   <- c(T, F, T)
dfa <- cbind(vNumeric, vCharacter, vLogical)
dfa  # Matrix of one data type
df <- as.data.frame(cbind(vNumeric, vCharacter, vLogical))
df  # Makes a data frame with three different data types
o1 <- c(1, 2, 3)
o2 <- c("a", "b", "c", "d")
o3 <- c(T, F, T, T, F)
list1 <- list(o1, o2, o3)
list1
list2 <- list(o1, o2, o3, list1)  # Lists within lists!
list2
(coerce1 <- c(1, "b", TRUE))
# coerce1  # Parenthese around command above make this moot
typeof(coerce1)
(coerce2 <- 5)
typeof(coerce2)
(coerce3 <- as.integer(5))
typeof(coerce3)
(coerce4 <- c("1", "2", "3"))
typeof(coerce4)
(coerce5 <- as.numeric(c("1", "2", "3")))
typeof(coerce5)
(coerce6 <- matrix(1:9, nrow= 3))
is.matrix(coerce6)
(coerce7 <- as.data.frame(matrix(1:9, nrow= 3)))
is.data.frame(coerce7)
# Clear environment
rm(list = ls())
# Clear console
cat("\014")  # ctrl+L
(x1 <- 1:3)
(y  <- 1:9)
# Combine variables
(df1 <- cbind.data.frame(x1, y))
typeof(df1$x1)
str(df1)
(x2  <- as.factor(c(1:3)))
(df2 <- cbind.data.frame(x2, y))
typeof(df2$x2)
str(df2)
x3  <- c(1:3)
df3 <- cbind.data.frame(x3, y)
(df3$x3 <- factor(df3$x3,
levels = c(1, 2, 3)))
typeof(df3$x3)
str(df3)
x4  <- c(1:3)
df4 <- cbind.data.frame(x4, y)
df4$x4 <- factor(df4$x4,
levels = c(1, 2, 3),
labels = c("macOS", "Windows", "Linux"))
df4
typeof(df4$x4)
str(df4)
x5  <- c(1:3)
df5 <- cbind.data.frame(x5, y)
(df5$x5 <- ordered(df5$x5,
levels = c(3, 1, 2),
labels = c("No", "Maybe", "Yes")))
df5
typeof(df5$x5)
str(df5)
# Clear environment
rm(list = ls())
# Clear console
cat("\014")  # ctrl+L
gc()
# Clear environment
rm(list = ls())
# Clear console
cat("\014")  # ctrl+L
# COLON OPERATOR ###########################################
# Can do ALT + - (ALT-dash) to auto put in this: "<- "
# Assigns number 0 through 10 to x1
x1 <- 0:10
x1
# Descending order
x2 <- 10:0
x2
?seq  # R help on seq
# Ascending values (duplicates 1:10)
(x3 <- seq(10))
# Specify change in values
(x4 <- seq(30, 0, by = -3))
# c = concatenate (or combine or collect)
?c  # R help on c
x5 <- c(5, 4, 1, 6, 7, 2, 2, 3, 2, 8)
x5
?scan  # R help on scan
x6 <- scan()  # After running this command, go to console
x6 <- scan()  # After running this command, go to console
# Hit return after each number
# Hit return twice to stop
x6
?rep  # R help on rep
x7 <- rep(TRUE, 5)
x7
# Repeats set
x8 <- rep(c(TRUE, FALSE), 5)
x8
# Repeats items in set
x9 <- rep(c(TRUE, FALSE), each = 5)
x9
# Clear environment
rm(list = ls())
# Clear console
cat("\014")  # ctrl+L
library(datasets)  # Load base packages manually
# Installs pacman ("package manager") if needed
if (!require("pacman")) install.packages("pacman")
# Use pacman to load add-on packages as desired
pacman::p_load(pacman, rio)
# From the official R documentation
browseURL("http://j.mp/2aFZUrJ")
# CSV
rio_csv <- import("~/Desktop/mbb.csv")
path <- "S:\Classes\Introduction to Statistics\RCourseFiles\CourseFiles\ImportingData_Datasets\"
path <- "S:\\Classes\\Introduction to Statistics\\RCourseFiles\\CourseFiles\\ImportingData_Datasets\\"
# CSV
rio_csv <- import(path + "mbb.csv")
path <- "S:\\Classes\\Introduction to Statistics\\RCourseFiles\\CourseFiles\\ImportingData_Datasets\\"
# CSV
rio_csv <- import(paste(path, "mbb.csv", sep=""))
head(rio_csv)
# TXT
rio_txt <- import(paste(path, "mbb.txt", sep=""))
head(rio_txt)
# Excel XLSX
rio_xlsx <- import(paste(path, "mbb.xlsx", sep=""))
head(rio_xlsx)
?View
View(rio_csv)
?View
View(rio_csv)
# Load a spreadsheet that has been saved as tab-delimited
# text file. Need to give complete address to file. This
# command gives an error on missing data but works on
# complete data.
r_txt1 <- read.table(paste(path, "mbb.txt", sep=""), header = TRUE)
# Load a spreadsheet that has been saved as tab-delimited
# text file. Need to give complete address to file. This
# command gives an error on missing data but works on
# complete data.
txt_path = paste(path, "mbb.txt", sep="")
r_txt1 <- read.table(txt_path, header = TRUE)
# This works with missing data by specifying the separator:
# \t is for tabs, sep = "," for commas. R converts missing
# to "NA"
r_txt2 <- read.table(txt_path,
header = TRUE,
sep = "\t")
# CSV FILES
# Don't have to specify delimiters for missing data
# because CSV means "comma separated values"
trends.csv <- read.csv(paste(path, "mbb.csv", sep=""), header = TRUE)
# Clear environment
rm(list = ls())
# Clear packages
p_unload(all)  # Remove all add-ons
# Clear console
cat("\014")  # ctrl+L
library(datasets)  # Load base packages manually
# Installs pacman ("package manager") if needed
if (!require("pacman")) install.packages("pacman")
# Use pacman to load add-on packages as desired
pacman::p_load(pacman, tidyverse)
?mtcars
head(mtcars)
cars <- mtcars[, c(1:4, 6:7, 9:11)]  # Select variables
head(cars)
# Save hierarchical clustering to "hc." This codes uses
# pipes from dplyr.
hc <- cars   %>%  # Get cars data
dist   %>%  # Compute distance/dissimilarity matrix
hclust      # Computer hierarchical clusters
plot(hc)          # Plot dendrogram
rect.hclust(hc, k = 2, border = "gray")
rect.hclust(hc, k = 3, border = "blue")
rect.hclust(hc, k = 4, border = "green4")
rect.hclust(hc, k = 5, border = "darkred")
# Clear environment
rm(list = ls())
# Clear packages
p_unload(all)  # Remove all add-ons
detach("package:datasets", unload = TRUE)  # For base
# Clear plots
dev.off()  # But only if there IS a plot
# Clear console
cat("\014")  # ctrl+L
# Packages I load every time; uses "pacman"
pacman::p_load(pacman, dplyr, GGally, ggplot2, ggthemes,
ggvis, httr, lubridate, plotly, rio, rmarkdown, shiny,
stringr, tidyr)
library(datasets)  # Load base packages manually
head(mtcars)
cars <- mtcars[, c(1:4, 6:7, 9:11)]  # Select variables
head(cars)
# For entire data frame ####################################
pc <- prcomp(cars,
center = TRUE,  # Centers means to 0 (optional)
scale = TRUE)   # Sets unit variance (helpful)
pc <- prcomp(~ mpg + cyl + disp + hp + wt + qsec + am +
gear + carb,
data = mtcars,
center = TRUE,
scale = TRUE)
# Get summary stats
summary(pc)
# Screeplot for number of components
plot(pc)
# Screeplot for number of components
plot(pc)
# Get standard deviations and rotation
pc
# See how cases load on PCs
predict(pc) %>% round(2)
# Biplot of first two components
biplot(pc)
# Clear environment
rm(list = ls())
# Clear packages
p_unload(all)  # Remove all add-ons
detach("package:datasets", unload = TRUE)  # For base
# Clear plots
dev.off()  # But only if there IS a plot
# Clear console
cat("\014")  # ctrl+L
library(datasets)  # Load base packages manually
# Installs pacman ("package manager") if needed
if (!require("pacman")) install.packages("pacman")
# Use pacman to load add-on packages as desired
pacman::p_load(pacman, caret, lars, tidyverse)
?USJudgeRatings
head(USJudgeRatings)
data <- USJudgeRatings
# Define variable groups
x <- as.matrix(data[, -12])
y <- data[, 12]
# Using variable groups
reg1 <- lm(y ~ x)
# Results
reg1           # Coefficients only
summary(reg1)  # Inferential tests
anova(reg1)            # Coefficients w/inferential tests
coef(reg1)             # Coefficients (same as reg1)
confint(reg1)          # CI for coefficients
resid(reg1)            # Residuals case-by-case
hist(residuals(reg1))  # Histogram of residuals
# Conventional stepwise regression
stepwise <- lars(x, y, type = "stepwise")
# Stagewise: Like stepwise but with better generalizability
forward <- lars(x, y, type = "forward.stagewise")
# LAR: Least Angle Regression
lar <- lars(x, y, type = "lar")
# LASSO: Least Absolute Shrinkage and Selection Operator
lasso <- lars(x, y, type = "lasso")
# Comparison of R^2 for new models
r2comp <- c(stepwise$R2[6], forward$R2[6],
lar$R2[6], lasso$R2[6]) %>%
round(2)
names(r2comp) <- c("stepwise", "forward", "lar", "lasso")
r2comp  # Show values of R^2
# Clear environment
rm(list = ls())
# Clear packages
p_unload(all)  # Remove all add-ons
detach("package:datasets", unload = TRUE)  # For base
# Clear plots
dev.off()  # But only if there IS a plot
# Clear console
cat("\014")  # ctrl+L
# Installs pacman ("package manager") if needed
if (!require("pacman")) install.packages("pacman")
# Use pacman to load add-on packages as desired
pacman::p_load(pacman, rio, psych)
p_help(psych, web = F)  # Opens help in R Viewer
# set our working directory
setwd("S:\\Classes\\Introduction to Statistics\\Project1\\")
# set this to be the path to where all of our data lives
BASE_PATH <- ".\\data\\pre_proc_data\\"
# get data for images in the train set & take a sneak peek in console
img_data_train <- import(paste(BASE_PATH, "train\\image_data.csv", sep=""))
head(img_data_train)
# get data for objects in the train set & take a sneak peek in console
obj_data_train <- import(paste(BASE_PATH, "train\\object_data.csv", sep=""))
head(obj_data_train)
# get data for images in the test set & take a sneak peek in console
img_data_test <- import(paste(BASE_PATH, "test\\image_data.csv", sep=""))
head(img_data_test)
# get data for objects in the test set & take a sneak peek in console
obj_data_test <- import(paste(BASE_PATH, "test\\object_data.csv", sep=""))
head(obj_data_test)
# get data for images in the validation set & take a sneak peek in console
img_data_validation <- import(paste(BASE_PATH, "validation\\image_data.csv", sep=""))
head(img_data_validation)
# get data for objects in the train set & take a sneak peek in console
obj_data_validation <- import(paste(BASE_PATH, "validation\\object_data.csv", sep=""))
head(obj_data_validation)
## OBJECT COUNT IN IMAGES ##################################
describe(img_data_train$object_count) # get general details
describe(img_data_validation$object_count) # get general details
describe(img_data_test$object_count) # get general details
## OBJECT DATA ##############################
describe(obj_data_train) # get general details
describe(img_data_validation) # get general details
describe(img_data_test) # get general details
par(mfrow = c(2, 2))
plot(obj_data_train$x_norm, obj_data_train$y_norm,
xlim= c(0.0, 1.0),
ylim= c(0.0, 1.0),
col = "#cc0000",  # Hex code for datalab.cc red
pch = 19,         # Use solid circles for points
main = "Train: Normalized Location of Objects",
xlab = "X Location (Normalized)",
ylab = "Y Location (Normalized)")
plot(obj_data_validation$x_norm, obj_data_validation$y_norm,
xlim= c(0.0, 1.0),
ylim= c(0.0, 1.0),
col = "#cc0000",  # Hex code for datalab.cc red
pch = 19,         # Use solid circles for points
main = "Validation: Normalized Location of Objects",
xlab = "X Location (Normalized)",
ylab = "Y Location (Normalized)")
plot(obj_data_test$x_norm, obj_data_test$y_norm,
xlim= c(0.0, 1.0),
ylim= c(0.0, 1.0),
col = "#cc0000",  # Hex code for datalab.cc red
pch = 19,         # Use solid circles for points
main = "Test: Normalized Location of Objects",
xlab = "X Location (Normalized)",
ylab = "Y Location (Normalized)")
